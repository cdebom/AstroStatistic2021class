{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNHi_81RL-WZ"
      },
      "outputs": [],
      "source": [
        "#Estabelecendo o environment na máquina que usei\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpcySdJoL-Wd"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "\n",
        "import emcee\n",
        "import corner\n",
        "import nflows\n",
        "\n",
        "from astropy.io import fits\n",
        "from astropy.table import Table\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import torch\n",
        "import time\n",
        "import torch.utils.data\n",
        "from torch.nn import functional as F\n",
        "from nflows.flows.base import Flow\n",
        "from nflows.transforms.base import CompositeTransform\n",
        "from nflows import distributions, flows, transforms, utils\n",
        "from nflows.nn import nets\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:',device)\n",
        "print('Pytorch:',torch.__version__)\n",
        "if torch.cuda.is_available() is True:\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# Fixe as seeds para manter o código reprodutível\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVF8zNw4L-Wf"
      },
      "outputs": [],
      "source": [
        "# Usaremos esse dicionário para passar os parâmetros\n",
        "class dotdict(dict): \n",
        "  __getattr__ = dict.get\n",
        "  __setattr__ = dict.__setitem__\n",
        "  __delattr__ = dict.__delitem__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR0vaioSL-Wf"
      },
      "outputs": [],
      "source": [
        "# Parâmetros para o processo de treinamento\n",
        "args = dotdict()\n",
        "args.learning_rate = 1e-4 \n",
        "args.num_epochs = 8\n",
        "args.log_interval = 2\n",
        "args.batch_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVwEHn7TL-Wg"
      },
      "outputs": [],
      "source": [
        "# Esse é o catálogo onde estão os valores dos parâmetros para serem estimados\n",
        "catalog_path = '/home/dados229/Data/normalizing-flows/LensPop/lenses_DESc.txt'\n",
        "with open(catalog_path, 'r') as f:\n",
        "    catalog = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7po-4EFLL-Wg"
      },
      "outputs": [],
      "source": [
        "cabecalho = catalog[:1430]\n",
        "catalog = catalog[1430:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktdwSoxIL-Wh"
      },
      "outputs": [],
      "source": [
        "catalog_split = catalog.split('\\n')\n",
        "catalog_split = catalog_split[:-1]\n",
        "\n",
        "new_catalog_split = []\n",
        "for data in catalog_split:\n",
        "    line = data.split(' ')\n",
        "    for i, value in enumerate(line):\n",
        "        line[i] = float(value)\n",
        "    new_catalog_split.append(np.array(line))\n",
        "        \n",
        "catalog_dict = {int(data[0]): data[1:5] for data in new_catalog_split}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcz_VtmXL-Wh"
      },
      "outputs": [],
      "source": [
        "#Esses são os parâmetros em que estamos interessados, então vamos fazer o datafram apenas com eles\n",
        "\n",
        "columns = ['zl', #redshift da lente\n",
        "           'zs', #redshift da fonte\n",
        "           'b', #Raio de Einstein (arcseconds)\n",
        "           'sig_v' #Dispersão de velocidades da lente (km/s)\n",
        "          ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGrn5DyhL-Wi"
      },
      "outputs": [],
      "source": [
        "catalog_df = pd.DataFrame.from_dict(catalog_dict, orient='index', columns=columns)\n",
        "catalog_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42UGr1KEL-Wj"
      },
      "outputs": [],
      "source": [
        "data_path='/home/dados229/Data/normalizing-flows/LensPop/images/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJtYebDuL-Wj"
      },
      "outputs": [],
      "source": [
        "#Aqui vamos dar uma olhada nas imagens do nosso dataset\n",
        "\n",
        "with fits.open(data_path+\"100/image_i_SDSS.fits\", ignore_missing_end=True) as hdul:\n",
        "    data = hdul[0].data\n",
        "    \n",
        "plt.imshow(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoBGUO-eL-Wj"
      },
      "outputs": [],
      "source": [
        "#E ver com elas ficam flatenizadas, pois nossa IA só aceita entradas 1D\n",
        "\n",
        "flat_data = data.reshape(-1)\n",
        "flat_data.shape\n",
        "\n",
        "plt.plot(flat_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxPJoNa4L-Wk"
      },
      "source": [
        "Agora vamos carregar todo o dataset já flatenizando as imagens.\n",
        "\n",
        "Vamos montar uma versão com 3 bandas e outra com 1 única banda, a que possui os maiores picos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyQLM-skL-Wl"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "images_flatten = []\n",
        "unichannel_images_flatten = []\n",
        "\n",
        "for i in tqdm(catalog_df.index):\n",
        "    images_object = []\n",
        "    for j in ['i','g','r']:\n",
        "        with fits.open(data_path+f\"{i}/image_{j}_SDSS.fits\", ignore_missing_end=True) as hdul:\n",
        "            data = hdul[0].data\n",
        "            data_flat = data.reshape(-1)\n",
        "            images_object = np.concatenate((images_object, data_flat), axis=0)\n",
        "            \n",
        "            if j=='i':\n",
        "                unichannel_images_flatten.append(images_object)\n",
        "            \n",
        "            hdul.close()\n",
        "    \n",
        "    images_flatten.append(images_object)\n",
        "\n",
        "images = np.array(images_flatten)\n",
        "\n",
        "unichannel_images = np.array(unichannel_images_flatten)\n",
        "\n",
        "catalog_df['flat_images'] = images_flatten\n",
        "\n",
        "catalog_df['unichannel_flat_images'] = unichannel_images_flatten\n",
        "\n",
        "catalog_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwOx31b_L-Wl"
      },
      "outputs": [],
      "source": [
        "values = np.array(catalog_df[['zl', 'zs', 'b', 'sig_v']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3FQHSTkL-Wl"
      },
      "outputs": [],
      "source": [
        "#Essa é a cara dos nossos dados com 3 canais flatenizados\n",
        "\n",
        "plt.plot(images_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXbjZWmhL-Wm"
      },
      "source": [
        "# Treinamento com 3 bandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aee_N6hFL-Wm"
      },
      "outputs": [],
      "source": [
        "#Dividiremos o dataset em treino (75%) e teste (25%). Assim, poderemos avaliar os resultados fora dos dados de treinamento\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, values, test_size=0.25, random_state=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX8GEZ81L-Wm"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, images, values):\n",
        "        'Initialization'\n",
        "        self.images = images\n",
        "        self.values = values\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.values.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        x = values[index,:]\n",
        "        y = images[index,:]\n",
        "        return x, y\n",
        "\n",
        "training_set = Dataset(x_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(training_set,batch_size=args.batch_size,shuffle=True)\n",
        "\n",
        "test_set = Dataset(x_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=args.batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QftALTeeL-Wm"
      },
      "outputs": [],
      "source": [
        "#Os dados de treino são normalizados durante o treino para a rede convergir mais rapidamente\n",
        "x_std = train_loader.dataset.images.std((0,1))\n",
        "y_std = train_loader.dataset.values.std((0))\n",
        "print('x_std, y_std: ',x_std,y_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TUv3yylL-Wn"
      },
      "outputs": [],
      "source": [
        "#Aqui estamos criando a rede utilizada\n",
        "\n",
        "def PiecewiseRationalQuadraticCouplingTransform(iflow, input_size, hidden_size, context_size, num_blocks=1, activation=F.elu, num_bins=8):\n",
        "    return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
        "        mask=utils.create_alternating_binary_mask(input_size, even=(iflow % 2 == 0)),\n",
        "        transform_net_create_fn=(lambda in_features, out_features: nets.ResidualNet(in_features=in_features, \n",
        "        out_features=out_features, hidden_features=hidden_size,context_features=context_size, num_blocks=num_blocks,activation=activation)),\n",
        "        num_bins=num_bins, tails='linear', tail_bound=5, apply_unconditional_transform=False)\n",
        "\n",
        "def create_linear_transform(param_dim):\n",
        "    return transforms.CompositeTransform([\n",
        "        transforms.RandomPermutation(features=param_dim),\n",
        "        transforms.LULinear(param_dim, identity_init=True)])\n",
        "\n",
        "\n",
        "num_layers = 10 # quantidade de blocos com transformações lineares e aplicação de uma ResNet.\n",
        "mhidden_features = 1200 # número de neurônios em cada camada da ResNet\n",
        "base_dist = nflows.distributions.StandardNormal((4,)) # Essa é a distribuição inicial, uma Gaussiana 4D, pois possuimos 4 parâmetros \n",
        "\n",
        "\n",
        "transformsi = []\n",
        "for _ in range(num_layers):\n",
        "    transformsi.append(create_linear_transform(param_dim=4))\n",
        "    transformsi.append(PiecewiseRationalQuadraticCouplingTransform(_, 4, mhidden_features, context_size=17328)) \n",
        "transformsi.append(create_linear_transform(param_dim=4))\n",
        "transformflow = CompositeTransform(transformsi)\n",
        "\n",
        "\n",
        "model = Flow(transformflow, base_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFuEQWLjL-Wn"
      },
      "outputs": [],
      "source": [
        "args.y_size = 4 # Número de parâmetros\n",
        "args.x_size = train_loader.dataset.images[0].shape[0] # tamanho do input\n",
        "pytorch_total_params_grad = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Total params to optimize:', pytorch_total_params_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1v424QIL-Wn"
      },
      "outputs": [],
      "source": [
        "images = images.astype(np.double)\n",
        "values = values.astype(np.double)\n",
        "model = model.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhlAqEmYL-Wo"
      },
      "outputs": [],
      "source": [
        "#Aqui temos o treinamento da rede\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "train_loss_avg = []\n",
        "time0 = time.time()\n",
        "\n",
        "model.train()\n",
        "for epoch in tqdm(range(1, args.num_epochs + 1)):\n",
        "    train_loss = []\n",
        "    for batch_idx, (params, data) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        params = params.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = torch.tensor(data[:,:].cpu().numpy(), dtype=torch.float32)/x_std\n",
        "        y = torch.tensor(params[:,:].cpu().numpy(), dtype=torch.float32)/y_std\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = -model.log_prob(inputs=y.double(), context=x.double()).mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    train_loss_avg.append(np.mean(np.array(train_loss)))\n",
        "    if epoch % args.log_interval ==0: print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, loss.item()))\n",
        "\n",
        "print('Training: {0:2.2f} min'.format( (time.time()-time0)/60.) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OBtsFJbL-Wo"
      },
      "outputs": [],
      "source": [
        "#Vamos plotar a evolução da loss do treinamento\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('-log prob')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcIuSpKDL-Wo"
      },
      "source": [
        "## Avaliação de um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS3nRvvHL-Wo"
      },
      "outputs": [],
      "source": [
        "test_index = 50\n",
        "\n",
        "with fits.open(data_path+f\"{test_index}/image_i_SDSS.fits\", ignore_missing_end=True) as hdul:\n",
        "    data = hdul[0].data\n",
        "    data_flat = data.reshape(-1)\n",
        "    hdul.close()\n",
        "    \n",
        "image = data_flat.astype(dtype=np.double)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDpYpSnmL-Wo"
      },
      "outputs": [],
      "source": [
        "plt.plot(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtjikHYkL-Wp"
      },
      "outputs": [],
      "source": [
        "nsamples = 2000\n",
        "\n",
        "zl_samples = []\n",
        "zs_samples = []\n",
        "b_samples =[]\n",
        "sig_v_samples = []\n",
        "\n",
        "samples_histo = model.sample(1,context=torch.tensor(x_test[test_index]).repeat(nsamples,1)/x_std).data.cpu().numpy()\n",
        "\n",
        "samples_histo = samples_histo[:,0,:]*y_std\n",
        "\n",
        "print(y_test[test_index]) # esse são os valores reais dos parâmetros, que podem ser comparados com os valores que a rede estimou\n",
        "figure = corner.corner(samples_histo,labels=['zl','zs','b','sig_v'],truths=y_test[test_index], show_titles=True,plot_datapoints=False, fill_contours=True, bins=50, smooth=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnTSqT3L-Wp"
      },
      "source": [
        "## Avaliação do dataset de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6nzqiKEL-Wp"
      },
      "outputs": [],
      "source": [
        "test_medians = []\n",
        "test_sup = []\n",
        "test_inf = []\n",
        "\n",
        "slice = len(y_test)\n",
        "\n",
        "for test_index in tqdm(range(slice)):\n",
        "    nsamples = 2000\n",
        "\n",
        "    samples_histo = model.sample(1,context=torch.tensor(x_test[test_index]).repeat(nsamples,1)/x_std).data.cpu().numpy()\n",
        "    samples_histo = samples_histo[:,0,:]*y_std\n",
        "    \n",
        "    test_medians.append([np.quantile(samples_histo[:,0], 0.5),\n",
        "                       np.quantile(samples_histo[:,1], 0.5),\n",
        "                       np.quantile(samples_histo[:,2], 0.5),\n",
        "                       np.quantile(samples_histo[:,3], 0.5)])\n",
        "    \n",
        "    test_sup.append([np.quantile(samples_histo[:,0], 0.84) - np.quantile(samples_histo[:,0], 0.5),\n",
        "                       np.quantile(samples_histo[:,1], 0.84) - np.quantile(samples_histo[:,1], 0.5),\n",
        "                       np.quantile(samples_histo[:,2], 0.84) - np.quantile(samples_histo[:,2], 0.5),\n",
        "                       np.quantile(samples_histo[:,3], 0.84) - np.quantile(samples_histo[:,3], 0.5)])\n",
        "    \n",
        "    test_inf.append([np.quantile(samples_histo[:,0], 0.16) - np.quantile(samples_histo[:,0], 0.5),\n",
        "                       np.quantile(samples_histo[:,1], 0.16) - np.quantile(samples_histo[:,1], 0.5),\n",
        "                       np.quantile(samples_histo[:,2], 0.16) - np.quantile(samples_histo[:,2], 0.5),\n",
        "                       np.quantile(samples_histo[:,3], 0.16) - np.quantile(samples_histo[:,3], 0.5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMsMYhuFL-Wp"
      },
      "source": [
        "A métrica R2 vai nos dar uma ideia sobre a capacidade explicativa das previsões.\n",
        "\n",
        "Quanto mais próximo de 1, melhor a capacidade da rede de estimar o parâmetro. Valores negativos indicam a estimação do parâmetro está muito ruim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txqeXriHL-Wp"
      },
      "outputs": [],
      "source": [
        "r2 = r2_score(y_true = y_test, y_pred = test_medians, multioutput='raw_values')\n",
        "\n",
        "for i, j in zip(columns, r2):\n",
        "    print(f'R2 score de{i}: {r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do-v8FzuL-Wq"
      },
      "source": [
        "Para obtermos mais informações sobre a qualidade das previsões, vamos plotar, para cada parâmetro, a mediana e o desvio-padrão das previsões em relação ao valor real.\n",
        "\n",
        "Quanto mais a curva gerada se aproximar da reta x=y (que será plotada para efeitos de comparação), melhor.\n",
        "\n",
        "Olhando as imagens geradas, poderemos observar melhor o significado dos valores de r2 para cada parâmetro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IUSVN-XL-Wq"
      },
      "outputs": [],
      "source": [
        "zl_median = []\n",
        "zl_sup = []\n",
        "zl_inf = []\n",
        "\n",
        "zs_median = []\n",
        "zs_sup = []\n",
        "zs_inf = []\n",
        "\n",
        "b_median = []\n",
        "b_sup = []\n",
        "b_inf = []\n",
        "\n",
        "\n",
        "sig_v_median = []\n",
        "sig_v_sup = []\n",
        "sig_v_inf = []\n",
        "\n",
        "for i in range(len(test_medians)):\n",
        "    zl_median.append(test_medians[i][0])\n",
        "    zs_median.append(test_medians[i][1])\n",
        "    b_median.append(test_medians[i][2])\n",
        "    sig_v_median.append(test_medians[i][3])\n",
        "    \n",
        "    zl_sup.append(test_sup[i][0] + test_medians[i][0])\n",
        "    zs_sup.append(test_sup[i][1] + test_medians[i][1])\n",
        "    b_sup.append(test_sup[i][2] + test_medians[i][2])\n",
        "    sig_v_sup.append(test_sup[i][3] + test_medians[i][3])\n",
        "    \n",
        "    zl_inf.append(test_inf[i][0] + test_medians[i][0])\n",
        "    zs_inf.append(test_inf[i][1] + test_medians[i][1])\n",
        "    b_inf.append(test_inf[i][2] + test_medians[i][2])\n",
        "    sig_v_inf.append(test_inf[i][3] + test_medians[i][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPZv0SIIL-Wq"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(15,15))\n",
        "axs = figure.subplots(2, 2)\n",
        "\n",
        "y_test_0_sorted, zl_median_sorted = zip(*sorted(zip(y_test[:,0], zl_median)))\n",
        "_, zl_sup_sorted = zip(*sorted(zip(y_test[:,0], zl_sup)))\n",
        "_, zl_inf_sorted = zip(*sorted(zip(y_test[:,0], zl_inf)))\n",
        "\n",
        "axs[0,0].plot(y_test_0_sorted, zl_median_sorted, label=f'R2: {round(r2[0],2)}')\n",
        "\n",
        "axs[0,0].fill_between(x=y_test_0_sorted, y1=zl_sup_sorted, y2=zl_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[0,0].plot(np.linspace(y_test_0_sorted[0], y_test_0_sorted[-1], 100), \n",
        "         np.linspace(y_test_0_sorted[0], y_test_0_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "\n",
        "axs[0,0].set_title('zl (redshift da lente)', fontsize=15)\n",
        "axs[0,0].legend()\n",
        "\n",
        "y_test_1_sorted, zs_median_sorted = zip(*sorted(zip(y_test[:,1], zs_median)))\n",
        "_, zs_sup_sorted = zip(*sorted(zip(y_test[:,1], zs_sup)))\n",
        "_, zs_inf_sorted = zip(*sorted(zip(y_test[:,1], zs_inf)))\n",
        "\n",
        "axs[0,1].plot(y_test_1_sorted, zs_median_sorted, label=f'R2: {round(r2[1],2)}')\n",
        "\n",
        "axs[0,1].fill_between(x=y_test_1_sorted, y1=zs_sup_sorted, y2=zs_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[0,1].plot(np.linspace(y_test_1_sorted[0], y_test_1_sorted[-1], 100), \n",
        "         np.linspace(y_test_1_sorted[0], y_test_1_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "\n",
        "axs[0,1].set_title('zs (redshift da fonte)', fontsize=15)\n",
        "axs[0,1].legend()\n",
        "\n",
        "y_test_2_sorted, b_median_sorted = zip(*sorted(zip(y_test[:,2], b_median)))\n",
        "_, b_sup_sorted = zip(*sorted(zip(y_test[:,2], b_sup)))\n",
        "_, b_inf_sorted = zip(*sorted(zip(y_test[:,2], b_inf)))\n",
        "\n",
        "axs[1,0].plot(y_test_2_sorted, b_median_sorted, label=f'R2: {round(r2[2],2)}')\n",
        "\n",
        "axs[1,0].fill_between(x=y_test_2_sorted, y1=b_sup_sorted, y2=b_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[1,0].plot(np.linspace(y_test_2_sorted[0], y_test_2_sorted[-1], 100), \n",
        "         np.linspace(y_test_2_sorted[0], y_test_2_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "axs[1,0].set_title('b (Raio de Einstein)', fontsize=15)\n",
        "axs[1,0].legend()\n",
        "\n",
        "y_test_3_sorted, sig_v_median_sorted = zip(*sorted(zip(y_test[:,3], sig_v_median)))\n",
        "_, sig_v_sup_sorted = zip(*sorted(zip(y_test[:,3], sig_v_sup)))\n",
        "_, sig_v_inf_sorted = zip(*sorted(zip(y_test[:,3], sig_v_inf)))\n",
        "\n",
        "axs[1,1].plot(y_test_3_sorted, sig_v_median_sorted, label=f'R2: {round(r2[3],2)}')\n",
        "\n",
        "axs[1,1].fill_between(x=y_test_3_sorted, y1=sig_v_sup_sorted, y2=sig_v_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[1,1].plot(np.linspace(y_test_3_sorted[0], y_test_3_sorted[-1], 100), \n",
        "         np.linspace(y_test_3_sorted[0], y_test_3_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "\n",
        "axs[1,1].set_title('sig_v (distribuição de velocidades)', fontsize=15)\n",
        "axs[1,1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcckU95qL-Wq"
      },
      "source": [
        "# E se utilizássemos apenas 1 dos canais?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "DlvNFjYsL-Wr"
      },
      "source": [
        "A ideia dessa seção é fazer o mesmo que fizemos anteriormente, mas dessa vez utilizando apenas os dados do canal i. Ao final, teremos uma nova imagem como a que está acima e poderemos ver a influência de se ter mais canais para cada parâmetro sendo estimado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3phpO4kiL-Wr"
      },
      "outputs": [],
      "source": [
        "images = np.array(catalog_df['unichannel_flat_images'].to_numpy())\n",
        "\n",
        "t = []\n",
        "for i in images:\n",
        "    t.append(np.array(i))\n",
        "images = np.array(t)\n",
        "images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UNghP0ML-Wr"
      },
      "source": [
        "Como nosso objetivo é comparar os resultados, vamos manter o mesmo random_state na função train_test_split. Dessa forma, os dados referentes aos mesmos objetos serão utilizados no treino e no teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MD4wbsoL-Wr"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(images, values, test_size=0.25, random_state=50)\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, images, values):\n",
        "        'Initialization'\n",
        "        self.images = images\n",
        "        self.values = values\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.values.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        x = values[index,:]\n",
        "        y = images[index,:]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "training_set = Dataset(x_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(training_set,batch_size=args.batch_size,shuffle=True)\n",
        "\n",
        "test_set = Dataset(x_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=args.batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "209QEXj_L-Wr"
      },
      "outputs": [],
      "source": [
        "x_std = train_loader.dataset.images.std((0,1))\n",
        "y_std = train_loader.dataset.values.std((0))\n",
        "print('x_std, y_std: ',x_std,y_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nZHJQ_HL-Wr"
      },
      "outputs": [],
      "source": [
        "def PiecewiseRationalQuadraticCouplingTransform(iflow, input_size, hidden_size, context_size, num_blocks=1, activation=F.elu, num_bins=8):\n",
        "    return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
        "        mask=utils.create_alternating_binary_mask(input_size, even=(iflow % 2 == 0)),\n",
        "        transform_net_create_fn=(lambda in_features, out_features: nets.ResidualNet(in_features=in_features, \n",
        "        out_features=out_features, hidden_features=hidden_size,context_features=context_size, num_blocks=num_blocks,activation=activation)),\n",
        "        num_bins=num_bins, tails='linear', tail_bound=5, apply_unconditional_transform=False)\n",
        "\n",
        "def create_linear_transform(param_dim):\n",
        "    return transforms.CompositeTransform([\n",
        "        transforms.RandomPermutation(features=param_dim),\n",
        "        transforms.LULinear(param_dim, identity_init=True)])\n",
        "\n",
        "num_layers = 10\n",
        "mhidden_features = 1200 \n",
        "base_dist = nflows.distributions.StandardNormal((4,))\n",
        "\n",
        "transformsi = []\n",
        "for _ in range(num_layers):\n",
        "    transformsi.append(create_linear_transform(param_dim=4))\n",
        "    transformsi.append(PiecewiseRationalQuadraticCouplingTransform(_, 4, mhidden_features, context_size=5776)) \n",
        "transformsi.append(create_linear_transform(param_dim=4))\n",
        "transformflow = CompositeTransform(transformsi)\n",
        "\n",
        "model = Flow(transformflow, base_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH2rYIiuL-Wr"
      },
      "outputs": [],
      "source": [
        "args.y_size = 4 \n",
        "args.x_size = train_loader.dataset.images[0].shape[0]\n",
        "pytorch_total_params_grad = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Total params to optimize:', pytorch_total_params_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vj33rG6L-Ws"
      },
      "outputs": [],
      "source": [
        "images = images.astype(np.double)\n",
        "values = values.astype(np.double)\n",
        "model = model.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QDa9wt_L-Ws"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "train_loss_avg = []\n",
        "time0 = time.time()\n",
        "\n",
        "model.train()\n",
        "for epoch in tqdm(range(1, args.num_epochs + 1)):\n",
        "    train_loss = []\n",
        "    for batch_idx, (params, data) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        params = params.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = torch.tensor(data[:,:].cpu().numpy(), dtype=torch.float32)/x_std\n",
        "        y = torch.tensor(params[:,:].cpu().numpy(), dtype=torch.float32)/y_std\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = -model.log_prob(inputs=y.double(), context=x.double()).mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    train_loss_avg.append(np.mean(np.array(train_loss)))\n",
        "    if epoch % args.log_interval ==0: print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, loss.item()))\n",
        "\n",
        "print('Training: {0:2.2f} min'.format( (time.time()-time0)/60.) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIqoKjKQL-Ws"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_loss_avg)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('-log prob')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl-04e1oL-Ws"
      },
      "outputs": [],
      "source": [
        "r2 = r2_score(y_true = y_test, y_pred = test_medians, multioutput='raw_values')\n",
        "\n",
        "for i, j in zip(columns, r2):\n",
        "    print(f'R2 score de{i}: {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQizq3p1L-Ws"
      },
      "outputs": [],
      "source": [
        "test_medians = []\n",
        "test_sup = []\n",
        "test_inf = []\n",
        "\n",
        "slice = len(y_test)\n",
        "\n",
        "for test_index in tqdm(range(slice)):\n",
        "    nsamples = 2000\n",
        "\n",
        "\n",
        "    samples_histo = model.sample(1,context=torch.tensor(x_test[test_index]).repeat(nsamples,1)/x_std).data.cpu().numpy()\n",
        "\n",
        "    samples_histo = samples_histo[:,0,:]*y_std\n",
        "    \n",
        "    test_medians.append([np.quantile(samples_histo[:,0], 0.5),\n",
        "                       np.quantile(samples_histo[:,1], 0.5),\n",
        "                       np.quantile(samples_histo[:,2], 0.5),\n",
        "                       np.quantile(samples_histo[:,3], 0.5)])\n",
        "    \n",
        "    test_sup.append([np.quantile(samples_histo[:,0], 0.84) - np.quantile(samples_histo[:,0], 0.5),\n",
        "                       np.quantile(samples_histo[:,1], 0.84) - np.quantile(samples_histo[:,1], 0.5),\n",
        "                       np.quantile(samples_histo[:,2], 0.84) - np.quantile(samples_histo[:,2], 0.5),\n",
        "                       np.quantile(samples_histo[:,3], 0.84) - np.quantile(samples_histo[:,3], 0.5)])\n",
        "    \n",
        "    test_inf.append([np.quantile(samples_histo[:,0], 0.16) - np.quantile(samples_histo[:,0], 0.5),\n",
        "                       np.quantile(samples_histo[:,1], 0.16) - np.quantile(samples_histo[:,1], 0.5),\n",
        "                       np.quantile(samples_histo[:,2], 0.16) - np.quantile(samples_histo[:,2], 0.5),\n",
        "                       np.quantile(samples_histo[:,3], 0.16) - np.quantile(samples_histo[:,3], 0.5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-Xj0mtoL-Wt"
      },
      "outputs": [],
      "source": [
        "zl_median = []\n",
        "zl_sup = []\n",
        "zl_inf = []\n",
        "\n",
        "zs_median = []\n",
        "zs_sup = []\n",
        "zs_inf = []\n",
        "\n",
        "b_median = []\n",
        "b_sup = []\n",
        "b_inf = []\n",
        "\n",
        "\n",
        "sig_v_median = []\n",
        "sig_v_sup = []\n",
        "sig_v_inf = []\n",
        "\n",
        "for i in range(len(test_medians)):\n",
        "    zl_median.append(test_medians[i][0])\n",
        "    zs_median.append(test_medians[i][1])\n",
        "    b_median.append(test_medians[i][2])\n",
        "    sig_v_median.append(test_medians[i][3])\n",
        "    \n",
        "    zl_sup.append(test_sup[i][0] + test_medians[i][0])\n",
        "    zs_sup.append(test_sup[i][1] + test_medians[i][1])\n",
        "    b_sup.append(test_sup[i][2] + test_medians[i][2])\n",
        "    sig_v_sup.append(test_sup[i][3] + test_medians[i][3])\n",
        "    \n",
        "    zl_inf.append(test_inf[i][0] + test_medians[i][0])\n",
        "    zs_inf.append(test_inf[i][1] + test_medians[i][1])\n",
        "    b_inf.append(test_inf[i][2] + test_medians[i][2])\n",
        "    sig_v_inf.append(test_inf[i][3] + test_medians[i][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgvoUj8mL-Wt"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(15,15))\n",
        "axs = figure.subplots(2, 2)\n",
        "\n",
        "y_test_0_sorted, zl_median_sorted = zip(*sorted(zip(y_test[:,0], zl_median)))\n",
        "_, zl_sup_sorted = zip(*sorted(zip(y_test[:,0], zl_sup)))\n",
        "_, zl_inf_sorted = zip(*sorted(zip(y_test[:,0], zl_inf)))\n",
        "\n",
        "axs[0,0].plot(y_test_0_sorted, zl_median_sorted, label=f'R2: {round(r2[0],2)}')\n",
        "\n",
        "axs[0,0].fill_between(x=y_test_0_sorted, y1=zl_sup_sorted, y2=zl_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[0,0].plot(np.linspace(y_test_0_sorted[0], y_test_0_sorted[-1], 100), \n",
        "         np.linspace(y_test_0_sorted[0], y_test_0_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "\n",
        "axs[0,0].set_title('zl (redshift da lente)', fontsize=15)\n",
        "axs[0,0].legend()\n",
        "\n",
        "y_test_1_sorted, zs_median_sorted = zip(*sorted(zip(y_test[:,1], zs_median)))\n",
        "_, zs_sup_sorted = zip(*sorted(zip(y_test[:,1], zs_sup)))\n",
        "_, zs_inf_sorted = zip(*sorted(zip(y_test[:,1], zs_inf)))\n",
        "\n",
        "axs[0,1].plot(y_test_1_sorted, zs_median_sorted, label=f'R2: {round(r2[1],2)}')\n",
        "\n",
        "axs[0,1].fill_between(x=y_test_1_sorted, y1=zs_sup_sorted, y2=zs_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[0,1].plot(np.linspace(y_test_1_sorted[0], y_test_1_sorted[-1], 100), \n",
        "         np.linspace(y_test_1_sorted[0], y_test_1_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "\n",
        "axs[0,1].set_title('zs (redshift da fonte)', fontsize=15)\n",
        "axs[0,1].legend()\n",
        "\n",
        "y_test_2_sorted, b_median_sorted = zip(*sorted(zip(y_test[:,2], b_median)))\n",
        "_, b_sup_sorted = zip(*sorted(zip(y_test[:,2], b_sup)))\n",
        "_, b_inf_sorted = zip(*sorted(zip(y_test[:,2], b_inf)))\n",
        "\n",
        "axs[1,0].plot(y_test_2_sorted, b_median_sorted, label=f'R2: {round(r2[2],2)}')\n",
        "\n",
        "axs[1,0].fill_between(x=y_test_2_sorted, y1=b_sup_sorted, y2=b_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[1,0].plot(np.linspace(y_test_2_sorted[0], y_test_2_sorted[-1], 100), \n",
        "         np.linspace(y_test_2_sorted[0], y_test_2_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "axs[1,0].set_title('b (Raio de Einstein)', fontsize=15)\n",
        "axs[1,0].legend()\n",
        "\n",
        "y_test_3_sorted, sig_v_median_sorted = zip(*sorted(zip(y_test[:,3], sig_v_median)))\n",
        "_, sig_v_sup_sorted = zip(*sorted(zip(y_test[:,3], sig_v_sup)))\n",
        "_, sig_v_inf_sorted = zip(*sorted(zip(y_test[:,3], sig_v_inf)))\n",
        "\n",
        "axs[1,1].plot(y_test_3_sorted, sig_v_median_sorted, label=f'R2: {round(r2[3],2)}')\n",
        "\n",
        "axs[1,1].fill_between(x=y_test_3_sorted, y1=sig_v_sup_sorted, y2=sig_v_inf_sorted, alpha=0.3)\n",
        "\n",
        "axs[1,1].plot(np.linspace(y_test_3_sorted[0], y_test_3_sorted[-1], 100), \n",
        "         np.linspace(y_test_3_sorted[0], y_test_3_sorted[-1], 100),\n",
        "        linestyle='--')\n",
        "\n",
        "axs[1,1].set_title('sig_v (distribuição de velocidades)', fontsize=15)\n",
        "axs[1,1].legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "normalizing_flows.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}